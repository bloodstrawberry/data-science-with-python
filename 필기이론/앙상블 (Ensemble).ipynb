{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6e14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 개의 모델(weak learners)을 결합해 더 강력한 예측 성능을 얻는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427949fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66728e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "# 단일 모델로 동작할 수 있으나 일반화 성능이 떨어짐\n",
    "# Bootstrap은 Aggregation할 모델들의 다양성을 보이게 함\n",
    "# 불안정한 결과를 보이는 모델의 분산을 줄여 일반화 성능을 높임\n",
    "# Aggregation 모델의 수가 늘어나면 분산이 줄어들어 과적합 현상을 완화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38ff7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e41273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51caed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911c6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "# 경사부스팅의 기반 모델은 분류와 회귀 모두 상관 없이 회귀 모델을 사용\n",
    "# 부스팅 모델을 구성하는 모델의 수를 늘리면 평향이 커진다.\n",
    "# 부스팅은 순차적으로 모델을 결합, 병렬화가 어려움\n",
    "# Gradient Boost는 단계별로 손실 함수를 줄여 나가기 때문에 병렬 처리가 불가능하다.\n",
    "# AdaBoost는 다음 라운드의 모델을 학습할 때 오차가 큰 데이터 포인트의 비중을 높이고, 손실을 최소화하는 모델의 가중치를 구한다.\n",
    "# 틀린 것에 가중치를 주는 방식\n",
    "# 각 예측기가 선형 분류기였으면 결합된 최종 모델도 선형이다.\n",
    "# 중간에 weak learner를 써도 마지막에 비선형 결과가 나올 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과적합 해소를 위해 Base Estimator를 Weak Learner로 사용한다. (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c845d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1e92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ff21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래디언트 부스팅\n",
    "# 모든 학생의 실수 패턴을 분석해서 더 집중해서 가르치기\n",
    "# 이전 학습기의 잔차(residual)를 새로운 학습기의 목표값으로 사용 \n",
    "# 잔차에 대해 예측하는 모델을 추가하여 앙상블 모델을 구성 \n",
    "# 잔차는 loss function의 negative gradient와 같다. (잔차 = 실제값 - 예측값 : 자체가 gradient)\n",
    "# 오차를 줄이기 위해 잔차(residual)를 모델링하는 방식을 경사 하강법(gradient descent)으로 일반화한 방식 (잔차를 학습)\n",
    "\n",
    "# 손실 함수가 없으면 안됨.\n",
    "# 손실 함수의 음의 그래디언트(negative gradient)를 잔차로 사용하여 모델을 학습\n",
    "\n",
    "# 오차에 대해 모델을 만들어서 추가로 앙상블 모델을 구성한다.\n",
    "# LightGBM은 그래디언드 부스팅을 이용한 알고리즘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "# 문제를 많이 틀린 학생에게 더 집중해서 가르치기\n",
    "# 이전 학습기가 잘못 분류한 샘플에 가중치를 부여, 잔차를 직접 학습하지는 않음.\n",
    "# 다음 모델은 이전 모델이 틀린 샘플에 가중치를 준다.\n",
    "# weak learner는 랜덤 분류기보다 약간 더 성능이 좋아야 한다.\n",
    "# 분류 모델들의 confidence level에 따라 가중합하여 최종 결정을 내린다. (신뢰도에 따라 가중치를 줘서 최종 예측)\n",
    "# SVM을 내부 모델로 사용할 수 있다. (일반적으로는 Decision Stump = 깊이 1짜리 트리를 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# XGBoost는 Gradient Boosting의 개선 버전\n",
    "# Gradient Boost에 비해 규제(L1, L2)가 가능하고 병렬처리도 가능하여 속도가 빠르다.\n",
    "# 잔차 예측 + 2차 도함수(헤시안) 사용 (정확도↑)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb47eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking\n",
    "# 여러 개의 서로 다른 모델(기본 모델들, base learners)의 예측 결과를 \n",
    "# 또 다른 모델(메타 모델, meta-learner)에 입력으로 넣어 최종 예측을 수행하는 기법\n",
    "# 잔차를 학습하지는 않음.\n",
    "\n",
    "# 여러 개의 기본 학습기(base learners)가 같은 훈련 데이터를 바탕으로 개별 예측을 수행함.\n",
    "# 각 base learner의 예측값을 모아서 새로운 데이터셋(2차 특징)을 생성.\n",
    "# 이 데이터를 이용해 메타 학습기(meta learner)를 학습시키고, 최종 예측은 이 메타 모델이 수행.\n",
    "\n",
    "훈련 데이터\n",
    "   ↓\n",
    "[Model A]  → \n",
    "[Model B]  →  → 예측값 → 메타 모델 → 최종 예측\n",
    "[Model C]  →\n",
    "\n",
    "\n",
    "# 과적합 방지를 위해 base 모델의 예측값은 반드시 훈련 데이터가 아닌 검증 데이터에서 생성해야 함 (즉, Out-of-Fold 예측).\n",
    "# 메타 모델이 base 모델의 잘못된 학습을 과대 평가하지 않도록 주의해야 함.\n",
    "# 계산 비용이 큼 (여러 모델 학습 필요).\n",
    "\n",
    "# K-fold stacking: 훈련 데이터를 K개의 폴드로 나누어, 각 폴드에 대해 예측값을 생성.\n",
    "# Blending: 훈련 데이터를 훈련/홀드아웃 세트로 나누고, 홀드아웃 예측값으로 메타 모델을 학습."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
